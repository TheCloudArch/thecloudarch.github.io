1
00:00:19,080 --> 00:00:21,720
Chris Goosen: Welcome to the
cloud architects podcast, a

2
00:00:21,720 --> 00:00:25,560
podcast about cloud technology
and the people using it.

3
00:00:26,860 --> 00:00:28,990
Nicolas Blank: The cloud
architects podcast is sponsored

4
00:00:28,990 --> 00:00:32,920
by Kemp Technologies. Choose
Kemp to optimize your multi

5
00:00:32,920 --> 00:00:35,950
cloud application deployments
and simplify multi cloud

6
00:00:35,950 --> 00:00:39,790
application management. A single
pane of glass for application

7
00:00:39,790 --> 00:00:44,350
delivery, Kemp provides a 360
degree view of your entire

8
00:00:44,350 --> 00:00:48,700
application environment, and
even third party ADCs. Download

9
00:00:48,730 --> 00:00:52,180
Kemp360 for free today at
Kemptechnologies.com

10
00:00:56,560 --> 00:01:00,160
Hello and welcome to another
episode of the cloud architects

11
00:01:00,160 --> 00:01:06,010
podcast. I'm here today with my
co host Chris Goosen. Hello. And

12
00:01:06,010 --> 00:01:09,190
we have a very special guest
today is going to talk about

13
00:01:09,190 --> 00:01:12,160
something that's wonderfully
contentious. And I'm going to

14
00:01:12,160 --> 00:01:14,470
ask Chris to introduce our guest
twice today.

15
00:01:14,550 --> 00:01:16,470
Chris Goosen: Yeah, absolutely.
I'm excited about this topic

16
00:01:16,470 --> 00:01:18,960
actually. And I'm super excited
about the person we speaking to.

17
00:01:18,960 --> 00:01:23,730
So we are joined by Ashwin pal,
who is the director of cyber

18
00:01:23,730 --> 00:01:27,030
security for the APAC region at
unisys. And Ashwin, I actually

19
00:01:27,030 --> 00:01:31,440
go back really long time he was
the really the the reason I got

20
00:01:31,440 --> 00:01:34,080
interested in cyber security in
the first place. And so super

21
00:01:34,080 --> 00:01:35,490
excited to have you actually
welcome.

22
00:01:36,570 --> 00:01:40,380
Ashwin Pal: Thank you, Chris.
Great to be on this and great to

23
00:01:40,380 --> 00:01:42,480
reconnect man. It's it's been
it's been too long.

24
00:01:42,509 --> 00:01:44,549
Chris Goosen: It has it's been,
it's been a really long time. I

25
00:01:44,549 --> 00:01:46,529
think we could probably trace
back the last time we saw each

26
00:01:46,529 --> 00:01:49,139
other at some pub somewhere in
the rocks. But we won't go into

27
00:01:49,139 --> 00:01:53,309
that on this episode. So as a
means of introduction, do you

28
00:01:53,309 --> 00:01:56,609
want to tell the folks listening
a little bit about yourself and

29
00:01:56,609 --> 00:01:57,299
what it is you do?

30
00:01:57,930 --> 00:02:01,860
Ashwin Pal: Yeah, definitely. So
obviously, actually, Paul, and I

31
00:02:01,860 --> 00:02:05,610
run the cybersecurity business
for unisys across Asia back.

32
00:02:06,420 --> 00:02:11,940
Been in Australia now for 13
years. But I'm a kiwi and been

33
00:02:11,940 --> 00:02:16,590
doing this for 20 years now. So,
you know. Yeah, there isn't much

34
00:02:16,590 --> 00:02:19,140
that I haven't touched around
the Sophos cybersecurity space.

35
00:02:19,500 --> 00:02:21,810
Chris Goosen: Right. And, and
the the contentious subject that

36
00:02:21,810 --> 00:02:25,680
I think Nick was referring to is
a zero trust. Right. I think we

37
00:02:25,680 --> 00:02:31,470
hear that. We hear the phrase,
I'm soaking in zero trust a lot,

38
00:02:31,770 --> 00:02:35,640
lately. Yeah. Right. And I think
maybe, probably the best place

39
00:02:35,640 --> 00:02:38,970
to start is to probably unpack
exactly what that means. Because

40
00:02:38,970 --> 00:02:42,750
I think what I've found is in
talking to folks in talking to

41
00:02:42,750 --> 00:02:46,560
customers is there's a real
misconception around zero trust,

42
00:02:46,590 --> 00:02:48,990
you know, some folks think it's
a product. Okay, can I get me

43
00:02:48,990 --> 00:02:52,530
some of that zero trust? You
know, I think there's a real

44
00:02:52,530 --> 00:02:55,290
misunderstanding in the sort of
in the industry as to what it

45
00:02:55,290 --> 00:02:59,010
means and what it is who many
people have takes on it? So can

46
00:02:59,010 --> 00:03:01,890
you help us kind of unpack a
little bit about you know, at

47
00:03:01,890 --> 00:03:03,060
its core, what it is?

48
00:03:03,930 --> 00:03:07,710
Ashwin Pal: Yeah, totally man.
And, you know, you are pressing

49
00:03:07,710 --> 00:03:10,200
my buttons again, as as you used
to when you were a cleaner

50
00:03:10,200 --> 00:03:16,110
Australia. So, unfortunately,
you talk to anyone, right now,

51
00:03:16,110 --> 00:03:20,340
any other vendors, they've got
their own definition, right. And

52
00:03:20,820 --> 00:03:25,200
zero trust came out of
Forrester. So I get along really

53
00:03:25,200 --> 00:03:30,630
well with the APAC head of
security at Forrester. And, you

54
00:03:30,630 --> 00:03:33,150
know, she, she's running a
server right now. And one of the

55
00:03:33,300 --> 00:03:35,460
key points that if let's
actually come out from the

56
00:03:35,460 --> 00:03:40,200
survey with silos, it's just a
frustration around the multiple

57
00:03:40,260 --> 00:03:45,720
definitions of zero trust. So
zero trust, it's a philosophy,

58
00:03:45,990 --> 00:03:50,010
it is not a tool, okay? It's
it's not a particular

59
00:03:50,010 --> 00:03:53,670
technology, it's really a way of
actually implementing

60
00:03:53,730 --> 00:03:56,970
cybersecurity in an
organization. It was actually

61
00:03:56,970 --> 00:04:03,780
founded by john can divide. It's
for us to go in 2010. And,

62
00:04:04,080 --> 00:04:07,920
really, the principle
effectively states never trust,

63
00:04:08,430 --> 00:04:12,480
always verify. That's the key
thing thing. Now, what that

64
00:04:12,480 --> 00:04:17,880
effectively means is that any
user or application, any device

65
00:04:17,940 --> 00:04:20,820
that is actually coming into
your organization, any one of

66
00:04:20,820 --> 00:04:24,450
those needs to be authenticated
properly, needs to be

67
00:04:24,480 --> 00:04:28,350
authorized, you need to
implement very, very limited

68
00:04:28,380 --> 00:04:32,610
role based access controls on a
need to know basis only. And

69
00:04:32,610 --> 00:04:35,280
you're consistently
authenticating, authorizing

70
00:04:35,280 --> 00:04:39,660
them. And at the same time,
you're using a myriad of other

71
00:04:39,690 --> 00:04:43,800
technologies to make sure that
you are watching the behavior.

72
00:04:44,010 --> 00:04:47,460
You're watching their security
posture, you're looking at other

73
00:04:47,460 --> 00:04:51,600
contexts and other data sets,
things like threat intelligence

74
00:04:51,600 --> 00:04:54,720
information, you know, through
things like security, log

75
00:04:54,720 --> 00:04:58,740
monitoring, etc. To make sure if
anything actually goes bump, you

76
00:04:58,740 --> 00:05:00,810
actually kick them off in order
grammatically that should

77
00:05:00,810 --> 00:05:06,450
actually happen by itself. Now,
interestingly enough, about two

78
00:05:06,450 --> 00:05:11,610
weeks ago now, this has actually
released its paper on zero trust

79
00:05:11,640 --> 00:05:14,520
architecture. And they have
discussed a lot of these

80
00:05:14,520 --> 00:05:17,850
concepts. Now one of the
concepts that actually they have

81
00:05:17,850 --> 00:05:21,690
talked about in there is one of
control plane and data plane.

82
00:05:22,230 --> 00:05:25,620
Right. So data plane is
basically so you segregate both

83
00:05:25,620 --> 00:05:28,560
areas. data plane is actually
where the data travels control

84
00:05:28,560 --> 00:05:32,730
planes effectively, we actually
implement access controls or

85
00:05:32,730 --> 00:05:35,820
authorization controls. And
that's how you actually control

86
00:05:35,820 --> 00:05:39,330
access to the underlying data.
And you're basically segregating

87
00:05:39,330 --> 00:05:43,620
both of those. So when you talk
about zero trust, there are

88
00:05:43,620 --> 00:05:46,710
multiple ways of implementing
it. The segregation that is that

89
00:05:46,710 --> 00:05:50,280
I'm talking about, micro
segmentation is one of those

90
00:05:50,610 --> 00:05:53,790
that I'm most familiar with, to
be honest. But at the end of the

91
00:05:53,790 --> 00:05:57,450
day, so long as there is a
separation between the control

92
00:05:57,450 --> 00:06:01,350
plane and the data plane, and
you are consistently authorizing

93
00:06:01,350 --> 00:06:04,980
authenticating your users, your
applications, your devices,

94
00:06:05,010 --> 00:06:08,400
establishing the trust,
maintaining that trust, through

95
00:06:08,400 --> 00:06:12,930
constant posture, checking,
behavior, monitoring, and

96
00:06:12,930 --> 00:06:16,740
encrypting the data on the wire,
that's quite important as well.

97
00:06:17,130 --> 00:06:20,730
That's what zero trust will give
you. And as I say, you can hear

98
00:06:20,730 --> 00:06:24,420
it in what I'm saying. It is not
a tool of technology, it is a

99
00:06:24,420 --> 00:06:27,930
philosophy. And it really
depends on how you implement

100
00:06:27,930 --> 00:06:30,090
that in your implement that in
your own environment.

101
00:06:31,620 --> 00:06:34,590
Nicolas Blank: As well, Ashlyn,
I have to ask, I think it's

102
00:06:34,590 --> 00:06:37,860
really easy for us who have been
around for a long time to hear

103
00:06:37,890 --> 00:06:42,720
authentication and assume
identity, right? So you approach

104
00:06:42,750 --> 00:06:45,960
my application or my service,
and we're going to constantly

105
00:06:45,960 --> 00:06:48,630
ask you to verify who you are
through whatever methodology

106
00:06:48,630 --> 00:06:53,850
that we have. However, what is
zero trust mean, when we get to

107
00:06:53,850 --> 00:06:59,580
the the actual wire of data on
the wire? And how do we do zero

108
00:06:59,580 --> 00:07:04,200
trust in a physical environment
like a layer two, layer three,

109
00:07:04,200 --> 00:07:06,600
all the way up to layer seven,
where I'm talking zero trust

110
00:07:06,600 --> 00:07:10,980
across clouds, zero trust
towards my data center? How do I

111
00:07:10,980 --> 00:07:15,390
do that in a practical manner?
Without vendor locking?

112
00:07:16,410 --> 00:07:17,100
Impossible?

113
00:07:17,429 --> 00:07:20,039
Ashwin Pal: No, no, no, no, no,
no, no, no, it is absolutely

114
00:07:20,039 --> 00:07:23,609
possible. In fact, you should
abstract yourself from the

115
00:07:23,609 --> 00:07:26,549
vendors. So what you should
actually be doing is coming up

116
00:07:26,549 --> 00:07:30,959
with a strategy, which is, you
know, not technology focus. So

117
00:07:30,959 --> 00:07:34,709
the key components that actually
just spoke about, you know, you

118
00:07:34,709 --> 00:07:38,069
start off by thinking about what
those key components are, and

119
00:07:38,069 --> 00:07:41,099
how you would actually address
those key components, the core

120
00:07:41,099 --> 00:07:44,489
aspect here, and I like the fact
that you actually talked about

121
00:07:44,489 --> 00:07:47,849
the OSI model, because I would
actually walk away from that and

122
00:07:47,849 --> 00:07:51,929
actually talk about identity
itself. Identity is the new

123
00:07:51,929 --> 00:07:58,049
permanent, okay, because what is
a network? Where is your? Any

124
00:07:58,049 --> 00:08:02,339
answer? You can't, it doesn't
exist, you know, even with Cloud

125
00:08:02,459 --> 00:08:07,289
is anybody using one cloud, no
people multi cloud. So if you

126
00:08:07,289 --> 00:08:11,129
abstract yourself from the
network layer, or any technology

127
00:08:11,129 --> 00:08:15,059
for that matter, and focus on
the core principles of zero

128
00:08:15,059 --> 00:08:18,479
trust, which actually just
mentioned, at a strategic level,

129
00:08:18,749 --> 00:08:21,959
and then drill down, try and
achieve those outcomes, that's

130
00:08:21,959 --> 00:08:25,919
how you would actually achieve
zero trust. So if you took that

131
00:08:25,919 --> 00:08:32,129
identity piece, right, and anti
role based access control, just

132
00:08:32,129 --> 00:08:35,819
to those identities at an
abstract layer, it doesn't

133
00:08:35,819 --> 00:08:39,239
matter where the identity goes,
whether it's in your network,

134
00:08:39,359 --> 00:08:42,569
whether it's in your home, and
that's very, very relevant now.

135
00:08:43,109 --> 00:08:46,529
Particularly with respect to
working from home remote access,

136
00:08:46,529 --> 00:08:49,529
and all that stuff, or whether
it's actually in the cloud. And

137
00:08:49,529 --> 00:08:52,829
again, that is very important
now, as well, because a big part

138
00:08:52,829 --> 00:08:56,789
of this COVID working from home
shift, his people are working

139
00:08:56,789 --> 00:08:59,489
from home now. And a lot of the
data is actually now in the

140
00:08:59,489 --> 00:09:02,939
cloud, so you can access them
easily. Now, if you take the

141
00:09:02,939 --> 00:09:07,619
concept of their identity, which
stays the same, doesn't matter

142
00:09:07,619 --> 00:09:10,109
if it's on prem, whether it's in
the cloud, whether it's in your

143
00:09:10,109 --> 00:09:14,879
home, you tie your access to
that identity, and then enforce

144
00:09:15,119 --> 00:09:19,769
quite strict role based vennela
access controls. That's how you

145
00:09:19,769 --> 00:09:23,189
establish that based on the
identity, and that's how you

146
00:09:23,189 --> 00:09:24,419
restrict the access.

147
00:09:25,350 --> 00:09:29,970
Nicolas Blank: Okay, so what I'm
hearing you say, is, forget

148
00:09:29,970 --> 00:09:33,300
about the new trick. Just don't
trust it, and put something else

149
00:09:33,300 --> 00:09:37,860
on top, which is completely not
network related in any

150
00:09:38,130 --> 00:09:42,300
semblance. And what we're going
to pivot off on everything is

151
00:09:42,300 --> 00:09:43,020
identity.

152
00:09:43,559 --> 00:09:45,599
Ashwin Pal: Hundred percent,
because at the end of the day,

153
00:09:45,689 --> 00:09:48,569
how far does your network
extend? Can you extend it into

154
00:09:48,569 --> 00:09:51,869
Azure Cloud getting extended
into Google's cloud? Can you

155
00:09:51,869 --> 00:09:55,139
extend it into AWS? You can't
Can you extend it into people's

156
00:09:55,139 --> 00:09:58,979
homes? Not really, right? I
mean, we've got a situation now

157
00:09:59,039 --> 00:10:03,179
where there are multiple users
coming in from their home using

158
00:10:03,179 --> 00:10:07,139
the BYOD machines because not
everybody had a laptop. So when

159
00:10:07,139 --> 00:10:10,109
they actually went home, that
could not procure laptops. I

160
00:10:10,109 --> 00:10:12,989
know for a fact Dell, Lenovo,
all of these guys literally ran

161
00:10:13,019 --> 00:10:17,129
out of laptops, you couldn't buy
one. Okay? The option was, okay,

162
00:10:17,129 --> 00:10:21,149
cool. We'll set your home laptop
up to actually come in. So now

163
00:10:21,149 --> 00:10:24,539
all of a sudden, you're using
BYOD machines coming in over an

164
00:10:24,539 --> 00:10:27,989
insecure wireless connection
straight into your network.

165
00:10:28,079 --> 00:10:32,039
Yeah, you trust that connection?
Are you going to trust that

166
00:10:32,069 --> 00:10:35,459
endpoint? You see what you see
where I'm going with this

167
00:10:35,490 --> 00:10:36,150
Nicolas Blank: is a

168
00:10:36,330 --> 00:10:38,850
Ashwin Pal: challenge that you
are trying to address. And

169
00:10:38,850 --> 00:10:44,670
frankly, zero trust is the
answer. I love, you know, making

170
00:10:44,670 --> 00:10:46,950
predictions and just being
controversial with neuro

171
00:10:48,269 --> 00:10:50,219
cruces Easter, which is always
loving.

172
00:10:51,539 --> 00:10:54,089
One of the things that I've
actually been saying is,

173
00:10:55,409 --> 00:10:59,309
you know, zero trust, frankly,
last year was a pie in the sky

174
00:10:59,309 --> 00:11:03,779
concept. With the whole COVID
thing, it's basically giving it

175
00:11:03,779 --> 00:11:08,369
a shot in the arm. I can't I
guarantee you, within three

176
00:11:08,369 --> 00:11:11,939
years, a lot of organizations
would either be working towards

177
00:11:11,939 --> 00:11:15,749
zero trust, or would seriously
have it on the roadmap, zero

178
00:11:15,749 --> 00:11:18,839
trusted if you're not easy to
put into place. Because there

179
00:11:18,839 --> 00:11:21,869
are a number of challenges with
regards to compatibility with

180
00:11:21,869 --> 00:11:25,949
legacy environments, change
resistance, except, you know,

181
00:11:25,949 --> 00:11:30,209
time and cost all of that stuff.
So you'll need to eat the

182
00:11:30,209 --> 00:11:33,029
elephant. Hopefully, there
aren't any alpha blockers on

183
00:11:33,029 --> 00:11:38,339
this call. In bite sized chunks,
right. And that's, that's what

184
00:11:38,339 --> 00:11:41,999
you've got to do. But at the end
of the day, people simply have

185
00:11:41,999 --> 00:11:46,289
no choice now with the work from
home stuff with the move to

186
00:11:46,289 --> 00:11:49,829
cloud. And trust me, not
everybody's honey, anyone's

187
00:11:49,829 --> 00:11:52,049
going to come back into the
office full time anymore. The

188
00:11:52,049 --> 00:11:54,689
whole working from home stuff is
here to stay. So you've got to

189
00:11:54,689 --> 00:11:58,499
have, you've got to have a
response to that business

190
00:11:58,499 --> 00:12:01,829
challenge. And zero trust gives
you a pretty solid answer.

191
00:12:02,519 --> 00:12:05,849
Chris Goosen: So so. So what I
what I like about what you've

192
00:12:05,849 --> 00:12:10,259
said is because for the longest
time, I've been preaching that

193
00:12:10,289 --> 00:12:13,949
the identity is the foundational
component of any move to the

194
00:12:13,949 --> 00:12:17,609
cloud, right? Whenever we talk
to customers, I spoke to add a

195
00:12:17,639 --> 00:12:23,129
late customer call yesterday,
where they tend to get so

196
00:12:23,129 --> 00:12:27,119
fixated on the fact that they
need to move data from on prem

197
00:12:27,239 --> 00:12:29,729
to, you know, on premises
servers into a cloud

198
00:12:29,729 --> 00:12:32,849
environment, or they need to
enable this feature in in teams.

199
00:12:33,449 --> 00:12:36,119
And then you take the wind out
of the sails when you say, well,

200
00:12:36,119 --> 00:12:39,149
hang on, have you thought about
your identity? Oh, well, what do

201
00:12:39,149 --> 00:12:41,939
you mean? Well, people need to
be able to log into stuff. And

202
00:12:41,939 --> 00:12:44,549
that really is like, if it's
like building the house, right?

203
00:12:44,549 --> 00:12:49,379
You can't do it. Without doing
that foundational step. I kind

204
00:12:49,379 --> 00:12:51,539
of experienced the same thing,
because during COVID, I was

205
00:12:51,539 --> 00:12:54,719
trying to get a new laptop
myself. And I ended up having to

206
00:12:54,719 --> 00:12:58,199
use a use a Windows machine for
a little while because my Mac

207
00:12:58,199 --> 00:13:01,019
was an order, and it just took
forever for me to get it. Right.

208
00:13:01,019 --> 00:13:03,629
And it's the same situation like
vendors everywhere we're having

209
00:13:03,779 --> 00:13:06,209
having issues with this. And
unfortunately, you know, I was

210
00:13:06,209 --> 00:13:09,959
able to continue working with
without problem but you know,

211
00:13:10,319 --> 00:13:13,049
you're right. I think this this
new norm, this new shift, this

212
00:13:13,049 --> 00:13:15,899
is interesting. So I what I
wanted to pick on, or at least

213
00:13:15,929 --> 00:13:17,909
what I wanted to ask, I had a
couple of questions. Now the

214
00:13:17,909 --> 00:13:19,829
first one's gonna be a little
bit more controversial, right?

215
00:13:20,099 --> 00:13:25,649
You're up for? I've heard a lot
of talk about something that

216
00:13:25,649 --> 00:13:31,349
Google calls beyond cope. Yeah.
And and and it comes up a lot in

217
00:13:31,349 --> 00:13:35,429
these kind of zero trust type
conversations is just is this

218
00:13:35,429 --> 00:13:38,849
just Google's branding on some
of the same concept, though, is

219
00:13:38,849 --> 00:13:43,349
this something that they're
trying to turn into? Okay,

220
00:13:44,399 --> 00:13:47,879
Ashwin Pal: so that's in TNA.
Here we go. Now I'm going to

221
00:13:47,879 --> 00:13:52,589
start throwing four three letter
acronyms at you. That's zero

222
00:13:52,589 --> 00:13:57,329
trust, zero trust network
access, it is a it's a piece of

223
00:13:57,329 --> 00:14:01,619
the puzzle. It is absolutely not
completely zero trust on its

224
00:14:01,619 --> 00:14:07,259
own. Right. So effectively, what
beyond Corp does, and you've got

225
00:14:07,259 --> 00:14:12,839
z scalar, that have got a
similar technology as well. And,

226
00:14:13,259 --> 00:14:18,989
you know, effectively allow
secure remote access to certain

227
00:14:18,989 --> 00:14:23,369
applications. And the access is
tightly controlled, and the

228
00:14:23,369 --> 00:14:26,729
applications are not exposed
until such time that you

229
00:14:26,729 --> 00:14:29,729
actually need access to it.
That's effectively it in

230
00:14:29,729 --> 00:14:34,289
summary, right. So that's what
beyond Corp is, as I said

231
00:14:34,289 --> 00:14:38,249
earlier, if you go back to my
initial definition, zero trust

232
00:14:38,249 --> 00:14:44,009
needs to be holistic, it needs
to cover any device anywhere,

233
00:14:44,039 --> 00:14:49,379
any access methodology anytime,
right? So zip DNA is a piece of

234
00:14:49,379 --> 00:14:54,029
the pie. It is not the entire
pie. Okay. Very interesting. And

235
00:14:54,149 --> 00:14:57,269
the other, obviously, just add
to that, Chris. The other thing

236
00:14:57,269 --> 00:14:59,609
you've got to be very careful
about is vendor locking.

237
00:14:59,999 --> 00:15:03,659
Alright, so we spoke about that
earlier. And there was actually

238
00:15:03,659 --> 00:15:06,419
one of the questions, what
you've got to think about is

239
00:15:06,419 --> 00:15:09,089
that if you go with a particular
vendor, in this case, Google,

240
00:15:09,869 --> 00:15:13,109
you know, what are your
limitations? What can you do

241
00:15:13,109 --> 00:15:15,299
with them what you cannot do
without them? And is it going to

242
00:15:15,299 --> 00:15:19,469
lock you in and actually prevent
you from, you know, implementing

243
00:15:20,129 --> 00:15:22,709
the access from, you know, a
place where there might not

244
00:15:22,709 --> 00:15:25,559
actually exist or something like
that. So, again, you've got to

245
00:15:25,559 --> 00:15:28,469
be very careful about vendor
lock in. And as I say, don't

246
00:15:28,469 --> 00:15:32,939
start with a technology. Start
with what you want to achieve

247
00:15:32,999 --> 00:15:35,339
your and then go from there.
That's key.

248
00:15:36,269 --> 00:15:38,819
Chris Goosen: Yeah, definitely.
Sounds like it's so the second

249
00:15:38,849 --> 00:15:41,639
sort of, I guess, tack on
question to that. We're talking

250
00:15:41,639 --> 00:15:44,879
about the identity. It's the key
part of that. And you're you're

251
00:15:44,879 --> 00:15:48,659
essentially wanting to
authenticate based on all of the

252
00:15:48,659 --> 00:15:51,779
data points that you have,
right. And so when we, when we

253
00:15:51,779 --> 00:15:54,959
look at something like
conditional access, I mean, is

254
00:15:54,959 --> 00:15:57,449
it okay, just, you know, a
common a common situation, when

255
00:15:57,449 --> 00:15:59,909
you when you use conditional
access is to say, Well, if we

256
00:15:59,909 --> 00:16:03,779
can identify that we have a
known device, and we know the

257
00:16:03,779 --> 00:16:07,529
user user credentials, we trust
the user credentials, that we

258
00:16:07,529 --> 00:16:11,189
can be a little bit more relaxed
around how we, you know, sort of

259
00:16:11,189 --> 00:16:13,859
know two factor, for example,
right? And maybe we allow that

260
00:16:13,919 --> 00:16:16,529
person because it's a trusted
identity, it's a trusted device,

261
00:16:16,949 --> 00:16:20,039
we can allow them access to some
really sensitive data. Whereas

262
00:16:20,189 --> 00:16:25,049
that same user identity coming
from grandma's device, or, you

263
00:16:25,049 --> 00:16:28,919
know, a McDonald's network that
we've never seen, maybe we want

264
00:16:28,919 --> 00:16:32,639
to do a few more things, right.
So would you say that something

265
00:16:32,639 --> 00:16:35,609
like conditional access kind of
plays into this? Or is a key

266
00:16:35,609 --> 00:16:37,589
kind of building block for it?

267
00:16:38,490 --> 00:16:40,140
Ashwin Pal: Now, you've
absolutely hit the nail on the

268
00:16:40,140 --> 00:16:44,370
head there, right? So one of the
key issues around implementing

269
00:16:44,370 --> 00:16:50,250
zero trust is change resistance.
Because, you know, let me take

270
00:16:50,250 --> 00:16:54,960
it a step back as humans, as
organizations, we've always

271
00:16:54,960 --> 00:16:59,610
worked on the principle of
trust, right? For example, I'm

272
00:16:59,610 --> 00:17:02,040
sure you guys have all heard,
why would you hire someone if

273
00:17:02,040 --> 00:17:04,980
you don't actually trust them?
Right, their trust is

274
00:17:04,980 --> 00:17:08,100
effectively going completely
against that. Now, just to be

275
00:17:08,100 --> 00:17:11,010
clear, we're not saying that we
don't trust our employees, what

276
00:17:11,010 --> 00:17:14,700
we're actually saying is that
depending on their risk profile,

277
00:17:14,940 --> 00:17:18,570
and it is related to everything
you just said, you know, we want

278
00:17:18,570 --> 00:17:20,970
to be able to make sure that we
can verify before we can

279
00:17:20,970 --> 00:17:23,580
actually trust, it actually
makes sense. It's a bit like,

280
00:17:23,850 --> 00:17:26,310
you know, someone's actually
knocking on your front door.

281
00:17:26,400 --> 00:17:29,370
They've got a helmet on. Yeah.
What do you mean, do open the

282
00:17:29,370 --> 00:17:33,300
front door? Because at the
moment, the way network access

283
00:17:33,300 --> 00:17:36,300
actually works is that you know,
you come in and Chris, you know,

284
00:17:36,300 --> 00:17:39,600
this, you know, you either get a
wired or wireless connection

285
00:17:39,600 --> 00:17:42,930
onto your network. By the time
the login prompt comes up,

286
00:17:43,200 --> 00:17:47,490
you're already on the network,
correct? Yeah. What you've just

287
00:17:47,490 --> 00:17:50,880
done is you've actually let this
person with a helmet on into the

288
00:17:50,880 --> 00:17:53,100
house, you've opened the front
door, they walked into the

289
00:17:53,100 --> 00:17:59,730
house, and then you go, Oh, are
you? Okay? Right, what we're

290
00:17:59,730 --> 00:18:02,850
saying is basically keep the
door shot, make sure this person

291
00:18:02,850 --> 00:18:06,600
pulls the helmet off. And if you
know him, let him in it, don't

292
00:18:06,930 --> 00:18:10,080
keep them out there goes back to
what you just said, right? It's

293
00:18:10,080 --> 00:18:14,310
all risk based. So if the person
has an issue wearing a helmet,

294
00:18:14,520 --> 00:18:18,420
you can see their face happy
days, the risk is lower, you

295
00:18:18,420 --> 00:18:21,750
will actually have lower
controls in place. And you'll

296
00:18:21,750 --> 00:18:24,180
let them in because you've got
to be very, very careful and

297
00:18:24,180 --> 00:18:28,080
Cognizant, off, balancing user
experience with enhanced

298
00:18:28,080 --> 00:18:31,230
security. And that's always the
friction when it comes to cyber

299
00:18:31,230 --> 00:18:35,250
security. And that's where risk
based decision is very, very

300
00:18:35,250 --> 00:18:39,390
important. And then if the IRS,
you put further controls in

301
00:18:39,390 --> 00:18:45,480
place, and then once they're
actually in a key aspect of this

302
00:18:45,810 --> 00:18:49,560
is user and entity behavior
analysis. So you know,

303
00:18:49,560 --> 00:18:55,230
constantly monitoring, you know,
what this user is doing, what

304
00:18:55,230 --> 00:18:58,260
this device is doing, and
whether it actually matches what

305
00:18:58,260 --> 00:19:01,470
you would actually expect. And
if it doesn't, then hopefully

306
00:19:01,470 --> 00:19:04,710
you've got an automatic control
to either kill their access or

307
00:19:04,710 --> 00:19:08,250
limit their access, because that
could potentially mean that

308
00:19:08,250 --> 00:19:11,280
something's changed. You know,
we've got a man in the middle

309
00:19:11,280 --> 00:19:13,890
attack or something like that,
that has actually happened,

310
00:19:14,160 --> 00:19:18,960
obviously, with men in the
middle is, you know, it's a case

311
00:19:18,960 --> 00:19:23,220
of a valid user session has just
been hijacked by somebody who is

312
00:19:23,220 --> 00:19:27,750
nefarious. So you know, it is
risk based. But having said

313
00:19:27,750 --> 00:19:29,940
that, you still need to make
sure you actually have controls

314
00:19:29,940 --> 00:19:32,970
in place that you can constantly
monitor and control that very

315
00:19:32,970 --> 00:19:34,140
session until the end.

316
00:19:35,490 --> 00:19:39,300
Nicolas Blank: I want to just
ask them continuous question

317
00:19:39,300 --> 00:19:43,590
asked another contentious
question. And we spoke about

318
00:19:43,590 --> 00:19:46,320
identity, which is wonderful,
and I'm completely on that

319
00:19:46,320 --> 00:19:52,110
bandwagon. But then, both of you
very quickly, Chris, you started

320
00:19:52,110 --> 00:19:54,930
off talking about Microsoft
Azure Active Directory,

321
00:19:54,930 --> 00:19:58,440
conditional access and Ashwin,
you didn't bat an eyelid we just

322
00:19:58,680 --> 00:20:03,720
go with the flow. There is only
one identity provider and for

323
00:20:04,860 --> 00:20:09,870
four years for decades, in fact,
on premises, it made sense that

324
00:20:09,870 --> 00:20:13,290
we would use Microsoft Active
Directory. And our users are

325
00:20:13,290 --> 00:20:16,290
spoiled because we obfuscate
authorization and

326
00:20:16,290 --> 00:20:19,830
authentication, one package and
particularly with Kerberos. Life

327
00:20:19,830 --> 00:20:25,200
has just, you know, it's great
and it works. And but then we go

328
00:20:25,200 --> 00:20:27,660
to cloud, right? When whose
identity model are we talking

329
00:20:27,660 --> 00:20:31,500
about? We're still talking about
Microsoft identity. And so, let

330
00:20:31,500 --> 00:20:34,830
me ask you, Ashwin, I want to
ask you a question that a

331
00:20:34,830 --> 00:20:39,930
customer of mine asked me, he
said, I am concerned about

332
00:20:39,960 --> 00:20:45,180
vendor locking. Okay. But what
have you literally for 20 years

333
00:20:45,180 --> 00:20:48,600
been running in your data
centers as well, Microsoft

334
00:20:48,600 --> 00:20:52,350
Active Directory, but now, now
that I'm going to cloud, I'm

335
00:20:52,350 --> 00:20:55,830
concerned, because people have
told me about this thing called

336
00:20:55,860 --> 00:21:02,820
vendor locking. Yeah. And how do
I not get locked into Microsoft

337
00:21:02,850 --> 00:21:07,980
Azure Active Directory. However,
the other side of the coin is, I

338
00:21:07,980 --> 00:21:10,920
don't see anyone else doing
conditional access, and doing

339
00:21:10,920 --> 00:21:15,210
what Microsoft is doing, that
can at least from the outside,

340
00:21:15,210 --> 00:21:20,490
in, do that thing of Who are you
and where you from? At the same

341
00:21:20,520 --> 00:21:25,740
time before I'm going to let you
access my service? Right? So

342
00:21:27,240 --> 00:21:29,160
Ashwin Pal: how do we answer
that question to the market?

343
00:21:29,610 --> 00:21:34,590
Absolutely. And and so color
responses to that one?

344
00:21:35,610 --> 00:21:38,340
Obviously, you recall, I talked
about control plane and data

345
00:21:38,340 --> 00:21:41,520
plane, right, which is why we
started off and discussed the

346
00:21:41,910 --> 00:21:45,420
concept of the control plane,
the control plane effectively

347
00:21:45,420 --> 00:21:49,800
abstracts all the technologies,
right. So I have a control

348
00:21:49,800 --> 00:21:53,700
plane, which actually enforces
authentication authorization, I

349
00:21:53,700 --> 00:21:57,780
don't really care where the
identity store is where it's

350
00:21:57,780 --> 00:22:00,360
coming from. And that's
literally, you know, what, what

351
00:22:00,360 --> 00:22:05,340
we've actually what we've
actually done. So, you know, on

352
00:22:05,340 --> 00:22:11,100
prem use ad, as an example, I
don't care in the cloud, where

353
00:22:11,130 --> 00:22:13,740
you've got workloads, which
actually running and don't have

354
00:22:13,740 --> 00:22:17,880
connection to your ad,
internally, stand up a simple

355
00:22:17,880 --> 00:22:20,910
PKR infrastructure, basically
use certificate based

356
00:22:20,910 --> 00:22:24,270
authentication and that privacy
identities for those workloads.

357
00:22:24,510 --> 00:22:28,410
So as I said, you know, you've
got to be, whenever you start

358
00:22:28,410 --> 00:22:31,050
with this, you've got to start
with the concepts, the building

359
00:22:31,050 --> 00:22:33,870
blocks, and make sure you're
actually addressing the building

360
00:22:33,870 --> 00:22:38,640
blocks, as opposed to coming in
from a technology angle. And

361
00:22:38,640 --> 00:22:41,730
it's a very valid point, because
yes, you're right, Chris

362
00:22:41,730 --> 00:22:44,940
actually did go down the
technology path. But you would

363
00:22:44,940 --> 00:22:47,940
have noticed I keep pulling you
guys back away from technology.

364
00:22:47,940 --> 00:22:52,050
This is not a technology
conversation. This is a concept

365
00:22:52,050 --> 00:22:55,140
of philosophy called
conversation, whereby you'll

366
00:22:55,140 --> 00:22:58,740
actually have building blocks
from multiple providers that you

367
00:22:58,740 --> 00:23:02,640
would then pull together based
on the various components of

368
00:23:02,640 --> 00:23:07,110
zero trust into a into a
cohesive architecture that is

369
00:23:07,110 --> 00:23:11,070
then void. So makes sense. And
the example I've just given to

370
00:23:11,070 --> 00:23:14,910
you mixes up your own Pico
infrastructure, which you would

371
00:23:14,910 --> 00:23:19,380
have set up in the cloud as an
example. And on prem, I don't

372
00:23:19,380 --> 00:23:19,650
care.

373
00:23:20,580 --> 00:23:22,230
Nicolas Blank: I'm sure you guys
would have had the same

374
00:23:22,230 --> 00:23:26,370
discussions with customers when
we asked them. So let's have the

375
00:23:26,370 --> 00:23:29,640
security discussion. They say,
oh, mate, I'm secure because I

376
00:23:29,640 --> 00:23:32,730
have been to whoever that is,
right? And you think But hang

377
00:23:32,730 --> 00:23:36,120
on, that's a vendor, that's not
a framework. That's not a

378
00:23:36,120 --> 00:23:42,540
strategy. A. And when I used to
work for a migration vendor, it

379
00:23:42,540 --> 00:23:46,620
was also management vendor, I
used to talk to my customers and

380
00:23:46,620 --> 00:23:49,770
say that software is not a
process. software enables a

381
00:23:49,770 --> 00:23:55,170
process. And if you were talking
to a customer who's in cloud,

382
00:23:55,590 --> 00:23:59,400
wants to go to cloud, because to
be fair, we still have customers

383
00:23:59,400 --> 00:24:04,350
who are in cloud and immature,
right. And that's not from a

384
00:24:04,350 --> 00:24:08,430
security posture that's not from
a mental point of view, or

385
00:24:08,430 --> 00:24:11,280
customers who are still
evaluating about going to cloud

386
00:24:11,340 --> 00:24:15,420
for the very first time. What
would you say to those folks?

387
00:24:16,530 --> 00:24:19,950
Ashwin Pal: So I mean, in terms
of migrating to the cloud,

388
00:24:20,700 --> 00:24:24,390
you've got to you've got to get
the basics, right first, right,

389
00:24:24,390 --> 00:24:29,370
like and again, what is your
security posture? What is your

390
00:24:29,370 --> 00:24:32,670
risk posture? You know, do you
have policies around your

391
00:24:32,670 --> 00:24:35,700
minimum cybersecurity standards?
It's those things that come into

392
00:24:35,700 --> 00:24:39,540
play. I mean, if you look at any
one of the cloud providers, they

393
00:24:39,570 --> 00:24:42,330
provide security in the cloud,
some of it is pretty good. So

394
00:24:42,330 --> 00:24:44,880
often, it's pretty average. But
at the end of the day, it's a

395
00:24:44,880 --> 00:24:46,890
bit like going to a restaurant
and somebody gives you a menu,

396
00:24:47,160 --> 00:24:49,200
right? You've got all these
things that you actually can put

397
00:24:49,200 --> 00:24:52,200
in place or eat in this case,
it's up to you. It's like, what

398
00:24:52,200 --> 00:24:55,230
do you want to eat and how
hungry you are? It's exactly the

399
00:24:55,230 --> 00:24:58,470
same concept. It's actually not
about you can't and you

400
00:24:58,470 --> 00:25:01,920
shouldn't start with With a
technology conversation, it's

401
00:25:01,920 --> 00:25:05,490
going to be about your
requirements, etc, etc. And then

402
00:25:05,490 --> 00:25:09,450
if you are specifically looking
at zero trust, zero trust is one

403
00:25:09,450 --> 00:25:15,360
of those beautiful things, which
allows you to push your

404
00:25:15,360 --> 00:25:19,500
security, you know, based on
what you've talked about, into

405
00:25:19,500 --> 00:25:23,370
the cloud, that you can control
and is actually equivalent to on

406
00:25:23,370 --> 00:25:29,820
prem security. It abstracts you
from the security that is

407
00:25:29,820 --> 00:25:33,060
provided by the cloud provider.
I'm not saying you don't use it.

408
00:25:33,060 --> 00:25:36,060
In fact, you should. But you
know, the concept of security

409
00:25:36,060 --> 00:25:39,030
about multiple layers, right? So
if there is an issue with the

410
00:25:39,030 --> 00:25:43,020
cloud provider, there was an
issue with Amazon's cloud

411
00:25:43,020 --> 00:25:47,820
director, not long ago, right?
There was a vulnerability found

412
00:25:47,820 --> 00:25:50,430
now if you actually have the top
issue. But if you actually have

413
00:25:50,430 --> 00:25:53,160
a zero trust philosophy in
place, whereby you are

414
00:25:53,160 --> 00:25:56,850
controlling some of the security
yourself, and you have allowed

415
00:25:56,880 --> 00:26:01,260
minimum security based on strict
authentication, then you have a

416
00:26:01,260 --> 00:26:03,990
degree of protection. So those
are the things you actually have

417
00:26:03,990 --> 00:26:07,830
to think about. But it does come
down to, you know, what are your

418
00:26:07,830 --> 00:26:09,990
minimum standards around
security? what is acceptable

419
00:26:09,990 --> 00:26:11,130
from a risk perspective?

420
00:26:11,790 --> 00:26:14,280
Chris Goosen: I think, I think
it's a very important point,

421
00:26:14,280 --> 00:26:18,000
because I always pick on Nick,
because he has this phrase that

422
00:26:18,000 --> 00:26:20,550
he uses a lot called
requirements. elicitation,

423
00:26:20,580 --> 00:26:23,940
right, because you don't just
gather requirements up like

424
00:26:23,940 --> 00:26:27,570
they're, you know, puppies. And,
and so I think that's important,

425
00:26:27,570 --> 00:26:29,550
because I've had a lot of
conversations with customers,

426
00:26:29,550 --> 00:26:32,730
specifically security
conversations, where they go,

427
00:26:33,240 --> 00:26:35,610
Well, we need to secure this
thing. And you're like, Okay,

428
00:26:35,700 --> 00:26:38,910
well, what is your risk profile?
What what are the what's

429
00:26:38,910 --> 00:26:40,500
important to you, from a
security perspective? What are

430
00:26:40,500 --> 00:26:42,960
your requirements? And they go,
Well, I don't know the product.

431
00:26:42,960 --> 00:26:44,700
So I don't know what it can do.
I was like, Yeah, but that's,

432
00:26:44,940 --> 00:26:48,420
that's not the question I asked.
Right. The question is, what are

433
00:26:48,420 --> 00:26:51,150
your requirements? Because we
will tailor the product to do

434
00:26:51,150 --> 00:26:53,700
you know, the platform to do
what? What? meet those

435
00:26:53,700 --> 00:26:55,980
requirements. And so you get
into this sort of chicken and

436
00:26:55,980 --> 00:26:58,440
egg situation where someone
standing there with their arms

437
00:26:58,440 --> 00:27:01,020
crossed, going, Well, I need to
understand what the what

438
00:27:01,020 --> 00:27:03,270
features it has and what it can
do. Well, that's not really the

439
00:27:03,270 --> 00:27:05,610
discussion we're having. Yeah,
you need. Yeah, you know, we

440
00:27:05,610 --> 00:27:09,540
need to talk about what and I
think the other part of this is

441
00:27:09,540 --> 00:27:14,010
the concept of, you know, folks
tend to sometimes overthink this

442
00:27:14,010 --> 00:27:17,490
as well, right? They go into
this sort of analysis paralysis,

443
00:27:17,490 --> 00:27:20,310
where they want to end up
spending significantly more to

444
00:27:20,310 --> 00:27:23,520
protect something than it's
worth, right. And that in itself

445
00:27:23,520 --> 00:27:26,700
is a good idea, either. So it's
very important to understand,

446
00:27:26,850 --> 00:27:31,830
like, if you if you have things
that are not that critical, or

447
00:27:31,860 --> 00:27:35,940
our budget public, you know,
publicly categorize documents,

448
00:27:35,940 --> 00:27:39,570
for example, right? Does it make
sense for you to go down this

449
00:27:39,570 --> 00:27:43,410
like, super deep path of putting
all of these crazy security

450
00:27:43,410 --> 00:27:45,990
controls in place? When if that
thing isn't all that important

451
00:27:45,990 --> 00:27:49,440
to begin with? So understanding
that risk profile and

452
00:27:49,440 --> 00:27:51,600
understanding your requirements
very, very important to this

453
00:27:51,600 --> 00:27:54,750
whole discussion? That was my
long, roundabout way of getting

454
00:27:54,750 --> 00:27:55,350
to that point.

455
00:27:56,220 --> 00:27:58,590
Ashwin Pal: I completely agree.
Because that's really where the

456
00:27:58,590 --> 00:28:02,190
conversation actually needs to
start. If you're starting a

457
00:28:02,190 --> 00:28:06,810
security conversation with
technology, straightaway, the

458
00:28:06,810 --> 00:28:10,770
red flag goes up for me, because
yeah, you're gonna end up in a

459
00:28:10,770 --> 00:28:11,910
very wrong place.

460
00:28:12,540 --> 00:28:15,060
Chris Goosen: Yeah, you're
almost ticking feature boxes at

461
00:28:15,060 --> 00:28:17,730
that point, right, trying to
make Features Fit and go, Oh,

462
00:28:17,730 --> 00:28:20,010
well, we bought this license.
Let's make sure we're using

463
00:28:20,010 --> 00:28:22,170
every single one of those
checkboxes on the feature list.

464
00:28:22,620 --> 00:28:23,070
Yeah,

465
00:28:23,130 --> 00:28:27,810
Nicolas Blank: yeah. Yeah. And I
use the word elicitation.

466
00:28:27,960 --> 00:28:32,310
Because, well, for one thing was
taught to me by a business

467
00:28:32,310 --> 00:28:35,490
analyst who was fantastically
mature in this whole discussion.

468
00:28:35,490 --> 00:28:39,210
And how he illustrated it to me
was just like Chris says, this

469
00:28:39,210 --> 00:28:41,760
requirements online in the
ground, fast to pick up because,

470
00:28:41,760 --> 00:28:46,800
you know, otherwise, it would be
easy. And I roll when we engage

471
00:28:46,800 --> 00:28:52,620
with customers. And to me, our
customers is anyone who has this

472
00:28:52,620 --> 00:28:55,590
kind of conversation and could
be an internal or an external

473
00:28:55,590 --> 00:28:59,370
role. And for us as an external
role, we would go and wrestle

474
00:28:59,610 --> 00:29:03,210
with our customers. And it's
kind of like standing with our

475
00:29:03,570 --> 00:29:07,740
feet on the shoulders with a
pair of pliers trying to pull

476
00:29:07,740 --> 00:29:11,610
these requirements out of their
mouth, because it's not an easy

477
00:29:11,610 --> 00:29:15,510
discussion. It's a case of,
let's stop talking about this

478
00:29:15,510 --> 00:29:19,800
feature. Let's go back to what
is our need, what is what are we

479
00:29:19,800 --> 00:29:24,660
trying to defend? And very
often, when we talk to senior

480
00:29:24,660 --> 00:29:27,240
folks in the business,
especially the C suite folks

481
00:29:27,240 --> 00:29:32,010
who've been marketed to by
vendors, they will say, and I'll

482
00:29:32,010 --> 00:29:35,010
take the conversation back 20
years, I want a Blackberry.

483
00:29:35,250 --> 00:29:38,670
Well, why do you want a
blackberry? Well, it's cuz I

484
00:29:38,670 --> 00:29:42,570
went down to the pub was living
in London at the stage. So I

485
00:29:42,570 --> 00:29:46,320
went down to the pub, and Mata,
my friend who's a director works

486
00:29:46,320 --> 00:29:49,170
for this other company showed me
his Blackberry, it's cool. Now

487
00:29:49,170 --> 00:29:52,320
whatever is at the back end, can
you make that work? But that's

488
00:29:52,320 --> 00:29:57,960
not a requirement. So if if we
didn't walk them forward to do

489
00:29:57,960 --> 00:30:01,350
you need mobile collaboration?
Oh, yeah. Well, do you need

490
00:30:01,380 --> 00:30:05,400
secure mobile collaboration, and
then we take them down that path

491
00:30:05,700 --> 00:30:10,860
of let's articulate what it is
that we need that in your head

492
00:30:10,860 --> 00:30:13,470
is aligned to a feature. But you
saw that feature, and you

493
00:30:13,470 --> 00:30:17,700
thought, I need that. Because
that makes sense. But let's turn

494
00:30:17,700 --> 00:30:20,760
it back into something that we
can then defend, from a

495
00:30:20,760 --> 00:30:22,770
requirements point of view.
Yeah,

496
00:30:23,129 --> 00:30:26,459
Ashwin Pal: yeah, absolutely.
And, and that's the critical

497
00:30:26,459 --> 00:30:30,509
part, like every conversation
needs to start around, you know,

498
00:30:30,539 --> 00:30:34,079
what is your risk appetite?
Where are you in relation to

499
00:30:34,079 --> 00:30:38,249
your risk appetite? So what is
the current risk posture? And,

500
00:30:38,639 --> 00:30:43,379
you know, how, if there's a gap,
then how do we actually get

501
00:30:43,379 --> 00:30:47,579
there, and you know, you look at
the building blocks. Now,

502
00:30:47,759 --> 00:30:51,779
obviously, a key part of that is
you need to understand where

503
00:30:51,779 --> 00:30:55,289
your information assets are, and
how critical there because

504
00:30:55,289 --> 00:30:58,739
that's what's going to drive the
security controls. But you've

505
00:30:58,739 --> 00:31:02,819
also got to throw into the mix,
how the threat landscape is

506
00:31:02,819 --> 00:31:06,269
actually changing. So for
instance, if you look at what I

507
00:31:06,269 --> 00:31:10,919
just said, the massive the move
with working from home,

508
00:31:11,219 --> 00:31:14,519
effectively changed the threat
landscape, you know, for the

509
00:31:14,519 --> 00:31:18,449
worse, the whole environment
became a lot less controllable,

510
00:31:18,479 --> 00:31:22,829
because people were in their
homes, BYOD machines, insecure

511
00:31:22,829 --> 00:31:26,399
wireless, what I spoke about
earlier, then you've got to take

512
00:31:26,399 --> 00:31:29,429
a step back and think about
Alright, so how do I actually

513
00:31:29,429 --> 00:31:33,509
secure this? Like, what? What
access is required? What's the

514
00:31:33,509 --> 00:31:37,109
criticality of the data being
accessed? Where is this data?

515
00:31:37,439 --> 00:31:40,709
And you know, is zero trust a
good way to go? Those are the

516
00:31:40,709 --> 00:31:43,649
questions you would actually ask
before even going down the zero

517
00:31:43,649 --> 00:31:47,669
trust path. Going back to what
Chris actually said, and I know

518
00:31:47,669 --> 00:31:50,099
this is an absurd example. But
just go with it. You know, if

519
00:31:50,099 --> 00:31:52,289
you've actually got users that
are constantly coming in,

520
00:31:52,499 --> 00:31:56,159
looking at publicly available
information and doing stuff with

521
00:31:56,159 --> 00:31:59,699
it, they're who, you know, the
controls that you need to put in

522
00:31:59,699 --> 00:32:03,209
place is going to be much less
However, if they're coming in.

523
00:32:03,479 --> 00:32:07,229
And if they're actually playing
around with the formula for Coca

524
00:32:07,229 --> 00:32:11,129
Cola. Yeah, you might want to
think about zero trust in that

525
00:32:11,129 --> 00:32:11,489
case.

526
00:32:12,030 --> 00:32:16,830
Chris Goosen: Yeah. That makes
sense. So, um, last question, I

527
00:32:16,830 --> 00:32:20,520
guess, for me that I have is
resources, right? Let's say

528
00:32:20,610 --> 00:32:22,860
someone's listening to this,
they're going, Okay, I

529
00:32:22,860 --> 00:32:27,240
understand that zero trust is a
methodology or a, you know, a

530
00:32:27,240 --> 00:32:30,420
mindset as opposed to a product
that I can go buy off the shelf,

531
00:32:30,570 --> 00:32:33,270
right, no matter what vendors
are telling you, it is not a

532
00:32:33,270 --> 00:32:36,990
product. But I want to learn
more, I want to I want to

533
00:32:36,990 --> 00:32:40,650
understand how I can implement
this in my environment or for my

534
00:32:40,650 --> 00:32:43,740
organization, or there's some,
like other good resources

535
00:32:43,740 --> 00:32:47,550
available for folks to go and
skill up, or at least, you know,

536
00:32:47,610 --> 00:32:50,490
get better acquainted with this.
And something that isn't

537
00:32:50,490 --> 00:32:52,140
completely kind of marketing
skewed.

538
00:32:53,460 --> 00:32:56,370
Ashwin Pal: I was gonna say, I
could give up my email address,

539
00:32:56,370 --> 00:32:57,000
but I won't

540
00:33:00,150 --> 00:33:01,590
Unknown: expect Dilbert.

541
00:33:04,560 --> 00:33:08,370
Ashwin Pal: Jokes aside, two
things come to mind one. And

542
00:33:08,370 --> 00:33:12,330
thank God, NIST actually has
written the zero trust

543
00:33:12,390 --> 00:33:18,150
architecture paper, I would, I'm
in this to convert. And I go go

544
00:33:18,150 --> 00:33:21,660
to NIST for all of my
references. First off, that's

545
00:33:21,660 --> 00:33:23,820
probably the one that actually
referenced most, because it's

546
00:33:23,820 --> 00:33:29,100
obviously vendor agnostic,
right. And, frankly, I've

547
00:33:29,100 --> 00:33:31,740
written a white paper on this,
and you won't believe it. I

548
00:33:31,740 --> 00:33:34,110
literally wrote it over
Christmas, because I was getting

549
00:33:34,110 --> 00:33:37,620
so frustrated with seeing all
the different zero trust

550
00:33:37,620 --> 00:33:41,160
definitions coming up, I will
start I'm going to write my own

551
00:33:41,190 --> 00:33:43,200
paper and define this once and
for all

552
00:33:44,340 --> 00:33:47,550
Chris Goosen: the different time
to rule definitions. How am I

553
00:33:47,550 --> 00:33:50,250
going? How can I do know about
this? In all of my research for

554
00:33:50,250 --> 00:33:53,430
this episode, I didn't even know
about this. So is that something

555
00:33:53,430 --> 00:33:56,760
we can share in the shownotes?
It's publicly available paper?

556
00:33:57,630 --> 00:34:00,000
Ashwin Pal: Yeah, hundred
percent. Man, I'll, I'll flick

557
00:34:00,000 --> 00:34:05,190
it to you. After this and feel
free, feel free to distribute it

558
00:34:05,970 --> 00:34:09,180
with this, go for your life. So
it'll cover a lot of the

559
00:34:09,180 --> 00:34:12,660
concepts that I've actually
spoken about. And, you know,

560
00:34:12,660 --> 00:34:15,600
these days, like I've discussed,
not just your trust, but what

561
00:34:15,600 --> 00:34:18,510
are some of the challenges,
organizational challenges as

562
00:34:18,510 --> 00:34:22,080
well as the suggested approach.
So, you know, zero trust is it's

563
00:34:22,080 --> 00:34:27,930
hard to shoehorn or fit into a
legacy environment, but a lot of

564
00:34:27,930 --> 00:34:32,430
people are going through digital
transformation. A lot of people

565
00:34:32,430 --> 00:34:35,820
are actually going through cloud
migrations, as we discussed

566
00:34:35,820 --> 00:34:38,970
earlier. Those are great places
to start, because they're

567
00:34:38,970 --> 00:34:43,170
Greenfield opportunities and
projects. The thing is, you've

568
00:34:43,170 --> 00:34:46,920
got to make sure that you're
building security and zero trust

569
00:34:46,980 --> 00:34:51,300
is part of the entire process as
opposed to towards the end and

570
00:34:51,300 --> 00:34:55,470
somebody goes, Oh, what about
security? Okay.

571
00:34:57,720 --> 00:34:59,940
Yeah, yeah, that one before
Henry

572
00:35:00,840 --> 00:35:03,030
Chris Goosen: I use that example
on a daily basis pretty much

573
00:35:03,030 --> 00:35:05,160
because I've just discovered so
often

574
00:35:06,540 --> 00:35:09,180
Ashwin Pal: start, and then it's
a nightmare in a disaster,

575
00:35:09,180 --> 00:35:10,860
right? So you've got to make
sure you're actually putting

576
00:35:10,860 --> 00:35:14,850
security in from the start. I
mean, this is applicable to

577
00:35:14,850 --> 00:35:17,760
security as far as zero trust as
much as it is to broader

578
00:35:17,760 --> 00:35:21,900
security. When you're actually
thinking off a project, oh, my

579
00:35:21,900 --> 00:35:24,330
God, I don't understand why you
actually wouldn't have security

580
00:35:24,330 --> 00:35:26,880
as a key requirement. So when
you're doing that requirement,

581
00:35:27,090 --> 00:35:32,100
initial phase right up front,
before you're even going to like

582
00:35:32,100 --> 00:35:35,490
project funding business
guessing stage, it should be in

583
00:35:35,490 --> 00:35:38,760
there, you should be putting up
security against your

584
00:35:38,760 --> 00:35:42,600
requirements. And then going
ahead with the project, yeah.

585
00:35:43,320 --> 00:35:47,520
Mine to stick to your I trusted,
and effectively, it's almost a

586
00:35:47,520 --> 00:35:51,840
case of land and expand, okay,
you land here a trust into your

587
00:35:51,900 --> 00:35:55,020
your plan, migrations, your
digital transformation projects.

588
00:35:55,260 --> 00:35:57,870
And then as your legacy
environments are getting

589
00:35:57,870 --> 00:36:02,280
refreshed, you expand, you
expand, you expand, right, it's

590
00:36:02,280 --> 00:36:06,060
a multi year, multi phase
journey. But you've got to start

591
00:36:06,060 --> 00:36:11,250
somewhere. And as I said, a lot
of this discussed in my short,

592
00:36:11,250 --> 00:36:15,690
sharp paper, which which I'll
share after the call, and feel

593
00:36:15,690 --> 00:36:17,130
free to share that with the
audience.

594
00:36:17,250 --> 00:36:20,340
Nicolas Blank: Yeah, absolutely.
I just Chris, before we end

595
00:36:20,340 --> 00:36:23,880
this, this, just something I
want to draw an analogy to and

596
00:36:23,880 --> 00:36:30,180
Ashwin, I'd like for you to, to
agree or to correct. So if I'm

597
00:36:30,180 --> 00:36:33,930
putting my my Microsoft hat back
on, one of the things that I

598
00:36:33,930 --> 00:36:39,870
gain from docs.microsoft.com is
the cloud adoption framework for

599
00:36:39,870 --> 00:36:43,320
Azure. And one of the things I
enjoy about that is that it

600
00:36:43,380 --> 00:36:47,040
actually is vendor agnostic. So
for all of those Microsoft

601
00:36:47,040 --> 00:36:50,100
bashes out there, there's this
free framework called the cloud

602
00:36:50,100 --> 00:36:53,520
adoption framework for Azure.
And it addresses multi cloud.

603
00:36:53,820 --> 00:36:57,390
And one of the things that you
get from that is the concept of

604
00:36:57,390 --> 00:37:02,910
building a minimally viable
product from day one. And after

605
00:37:02,910 --> 00:37:06,060
we finish the envisioning phase
of we go to the business and

606
00:37:06,060 --> 00:37:09,960
say, What are you willing to
spend money on? Right? So what

607
00:37:09,960 --> 00:37:12,360
is this thing that cloud is
going to do for us in a

608
00:37:12,360 --> 00:37:15,840
measurable way, we actually
start with a governance

609
00:37:15,840 --> 00:37:21,030
framework. And governance starts
on day one with an MVP. Now I've

610
00:37:21,030 --> 00:37:23,820
taken that methodology, and I've
applied it to our productivity

611
00:37:23,820 --> 00:37:27,930
customers. So those folks going
to Office 365. And after we do

612
00:37:27,930 --> 00:37:32,730
this envisioning thing of the
cloud, we get the folks in the

613
00:37:32,730 --> 00:37:36,540
room. And then I look the
governance person, the eye and

614
00:37:36,540 --> 00:37:39,330
say, you've just become the most
important person in the room.

615
00:37:39,420 --> 00:37:43,890
Because without you, we can't
move a single bite to the cloud

616
00:37:43,920 --> 00:37:46,230
without being compliant to
whatever is important in your

617
00:37:46,230 --> 00:37:53,190
life. And this concept of going
to cloud in a secure manner,

618
00:37:53,190 --> 00:37:57,000
including zero trust doesn't
have to be the thing that says,

619
00:37:57,330 --> 00:38:00,480
Oh, it's going to take me 10
years to become secure, it means

620
00:38:00,600 --> 00:38:03,270
I'm going to start somewhere.
And then I'm going to be

621
00:38:03,300 --> 00:38:07,170
iteratively more mature, as I
take my minimally viable

622
00:38:07,170 --> 00:38:11,040
product, and iterate through
that as I become more mature in

623
00:38:11,070 --> 00:38:16,440
my security posture, and become
more mature in zero trust, so

624
00:38:16,440 --> 00:38:19,320
that I build something that I as
an organization can sustain.

625
00:38:20,130 --> 00:38:23,400
Ashwin Pal: Yep. Well said, and
I'm sure you guys know,

626
00:38:24,180 --> 00:38:28,530
Microsoft's my Microsoft is an
absolute zero trust convert.

627
00:38:29,310 --> 00:38:33,630
Yeah, they taught So actually,
they're on a multi year, zero

628
00:38:33,630 --> 00:38:37,740
trust, migration journey.
Period, right? And then betting

629
00:38:37,740 --> 00:38:41,040
in the cloud environment, just
just talking about Microsoft

630
00:38:41,070 --> 00:38:44,640
itself. So yeah, they're
absolutely sold on the concept.

631
00:38:44,670 --> 00:38:45,420
100%

632
00:38:46,140 --> 00:38:47,460
Chris Goosen: and if you're
listening to this, and you want

633
00:38:47,460 --> 00:38:51,090
to know, one thing you can do to
start today, turn on multi

634
00:38:51,090 --> 00:38:55,860
factor auth. Yes, that's right.
That's where you start.

635
00:38:56,580 --> 00:38:59,160
Unknown: Yeah, so I'll get off

636
00:38:59,160 --> 00:39:02,640
Chris Goosen: my soapbox now.
ashran it's been it's been an

637
00:39:02,640 --> 00:39:04,950
absolute pleasure catching up
again and having you having you

638
00:39:04,950 --> 00:39:08,040
talk to us about this. If folks
on the call want to reach out to

639
00:39:08,040 --> 00:39:12,060
you and and sort of get in touch
and talk about zero trust or

640
00:39:12,060 --> 00:39:14,400
anything else security related,
how can they How can they find

641
00:39:14,400 --> 00:39:14,640
you?

642
00:39:15,650 --> 00:39:18,170
Ashwin Pal: easiest thing to do
is just drop me drop me a line.

643
00:39:19,100 --> 00:39:24,080
And my email address is
ashwin.pal@unisys.com.

644
00:39:26,250 --> 00:39:28,470
Chris Goosen: Okay. And on the
socials, you're on LinkedIn.

645
00:39:28,470 --> 00:39:32,550
Right I'll while on LinkedIn, so
you can feel free to message me

646
00:39:32,580 --> 00:39:35,040
on LinkedIn as well. Okay,
excellent. I'll put we don't

647
00:39:35,040 --> 00:39:37,200
typically publish email
addresses but I'll publish a

648
00:39:37,200 --> 00:39:39,630
link to your LinkedIn profile.
Just keep the spam out of your

649
00:39:39,630 --> 00:39:46,410
inbox. You know, do Mike do
yeah. Yeah. Well, look, thank

650
00:39:46,410 --> 00:39:49,260
you very much. And yeah, we'll
look forward to possibly having

651
00:39:49,260 --> 00:39:50,580
you back again sometime in
future.

652
00:39:51,360 --> 00:39:54,090
Ashwin Pal: Yeah, man. No
worries. Any any any topic I'll

653
00:39:54,120 --> 00:39:55,200
give an opinion is you know.

654
00:39:56,700 --> 00:40:00,450
Nicolas Blank: Awesome. Thank
you Ashwin. Thanks. Eyes.

655
00:40:02,070 --> 00:40:05,070
Warren du Toit: Everyone. Before
you go, we just wanted to say

656
00:40:05,070 --> 00:40:07,320
thank you for listening. We
really enjoyed putting this

657
00:40:07,320 --> 00:40:10,650
podcast together for you every
two weeks, please visit us at

658
00:40:10,650 --> 00:40:13,710
the architects cloud.
Alternatively, drop us a tweet.

659
00:40:13,800 --> 00:40:16,530
We'd love to hear what you have
to say at the cloud Ark.

